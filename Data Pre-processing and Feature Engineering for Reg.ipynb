{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing and Feature Engineering \n",
    "### For regression model\n",
    "This Notebook will process the created data and create features for the regression model. The Following data sources are used: <br>\n",
    "dim_product: product fact table <br>\n",
    "streaming: aggregated daily streaming data (2018/6/1/-2020/7/2) <br>\n",
    "Note: New features can be added as a function. Ideally, the prepcrocessing and feature engineering are preferred to be completed in a ETL tool before data arrives in Sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "#!conda update pandas -y\n",
    "#!conda update s3fs -y\n",
    "#!pip install --upgrade numpy\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dim_product, filter product table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in dim_product as chunks and concat them together as the file is big\n",
    "bucket='wmg-streaming-prediction-dev/streaming_data_processed'\n",
    "filename = 'dim_product_all.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, filename)\n",
    "\n",
    "# Read in everything as string\n",
    "col_names = pd.read_csv(data_location, nrows=0).columns # get headers \n",
    "types_dict = {}\n",
    "types_dict.update({col: 'str' for col in col_names if col not in types_dict})\n",
    "\n",
    "csv_chunks = pd.read_csv(data_location, chunksize = 10000, dtype = types_dict, index_col=0)\n",
    "df_product = pd.concat(chunk for chunk in csv_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate tracks with more than 6 artists to keep artist related features clean. This step can be discussed \n",
    "#further with domain expert on how to clean up product fact table. \n",
    "num_names = pd.DataFrame(df_product.\n",
    "                         groupby('artist_id')['artist_display_name'].nunique()).reset_index()\n",
    "num_names.rename(columns={'artist_display_name':'num_artist_names'},inplace=True)\n",
    "num_names_lim = num_names[num_names['num_artist_names']<=6]\n",
    "df_product = df_product.merge(num_names_lim,\n",
    "                                       on='artist_id',\n",
    "                                       how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in streaming data, pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in provided streaming data\n",
    "##data anomolies were removed by Siva: stream date after release date, and stream date or release date in future\n",
    "\n",
    "bucket_streaming = 'wmg-streaming-prediction-dev/product-key-agg'\n",
    "filename = 'rna.gz'\n",
    "data_location = 's3://{}/{}'.format(bucket_streaming, filename)\n",
    "\n",
    "colnames = ['product_key', 'ISRC', 'release_date', 'stream_date', 'streams']\n",
    "types_dict = {'product_key': 'str'}\n",
    "\n",
    "df_streaming = pd.read_csv(data_location, compression='gzip')\n",
    "df_streaming.columns = colnames\n",
    "\n",
    "# Convert dates to datetime\n",
    "df_streaming['stream_date'] = pd.to_datetime(df_streaming['stream_date'],errors='coerce')\n",
    "df_streaming['release_date'] = pd.to_datetime(df_streaming['release_date'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total streams for each song\n",
    "df_streaming_agg = df_streaming.groupby(['ISRC']).agg({'streams': 'sum'}).reset_index().sort_values('streams', ascending=False)\n",
    "# Filter applied: Keep only songs that have > 1000 streams to limit the size of streaming data\n",
    "df_streaming_agg = df_streaming_agg[df_streaming_agg['streams'] > 1000]\n",
    "df_streaming_lim = df_streaming.merge(df_streaming_agg['ISRC'],on='ISRC',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter applied to product table to limit size \n",
    "df_streaming_lim_ids = pd.DataFrame(df_streaming_lim['ISRC'].unique(),columns=['ISRC'])\n",
    "df_product_lim = df_product.merge(df_streaming_lim_ids,\n",
    "                                  left_on = 'product_id',\n",
    "                                  right_on = 'ISRC',\n",
    "                                  how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data type and formatting\n",
    "df_product_lim['product_key'] = df_product_lim['product_key'].astype('str')\n",
    "df_streaming_lim['product_key'] = df_streaming_lim['product_key'].astype('str')\n",
    "df_product_lim.rename(columns = {'first_global_release_date': 'release_date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment and run this cell if OOM error\n",
    "del df_product\n",
    "del df_streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features\n",
    "The Following features will be created:\n",
    "artist total tracks released, artist total albums released,\n",
    "artist total streams, if the track is a collaboration track,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1 & 2: artist's total tracks released and total albums released"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_product(df):\n",
    "    #input: cleaned product dimension table\n",
    "    #output: artist's total number of tracks released (calculated as number of unique products associated by artist) \n",
    "    #and total number of album released (calculated as number of unique project_ids associated by artist).\n",
    "    df_product_numtracks = df.groupby(['artist_id']).agg({'product_key': 'count', 'project_key': pd.Series.nunique}).reset_index().sort_values('product_key', ascending=False)\n",
    "    df_product_numtracks.rename(columns={'product_key': 'artist_num_tracks'}, inplace=True)\n",
    "    df_product_numtracks.rename(columns={'project_key': 'artist_num_albums'}, inplace=True)\n",
    "\n",
    "    print('Shape of numtracks df:\\t{}'.format(df_product_numtracks.shape))\n",
    "    print('Number of unique artists in original df:\\t{}'.format(df['artist_id'].nunique()))\n",
    "    \n",
    "    return df_product_numtracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of numtracks df:\t(6605, 3)\n",
      "Number of unique artists in original df:\t6605\n"
     ]
    }
   ],
   "source": [
    "#the dataframe is saved in s3 \n",
    "#link to it 's3://wmg-streaming-prediction-dev/streaming_data_processed/tracks_albums_by_artist.csv'\n",
    "df_numtracks = create_features_product(df_product_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 3:  if the track is a Collaboration track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable for whether song is collaboration (calculated as if the track's name contains \"feat\")\n",
    "# filtered product table is saved in S3\n",
    "# link to the filtered product table: 's3://wmg-streaming-prediction-dev/streaming_data_processed/dim_product_limited.csv'\n",
    "df_product_lim['Collaboration'] = df_product_lim['product_title'].str.contains('Feat\\.', regex=True, flags=re.IGNORECASE).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 4: total streams per artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for number of streams by artist\n",
    "def create_num_streams_by_artist(stream_df, prod_df):\n",
    "    #input: cleaned streaming table and product dim table\n",
    "    #output: artist's total streams (calculated as number of sum of streams by artist).\n",
    "    df_artist = stream_df.merge(prod_df[['artist_id','product_id']],\n",
    "                         left_on='ISRC',\n",
    "                         right_on='product_id',\n",
    "                         how = 'inner')    \n",
    "    df_streaming_agg = df_artist.groupby(['artist_id']).agg({'streams': 'sum'}).reset_index().sort_values('streams', ascending=False)\n",
    "    df_streaming_agg.rename(columns={'Streams':'Total artist streams'},inplace=True)    \n",
    "    return df_streaming_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe (artist, total stream per artist) is saved in S3\n",
    "# link to the data: 's3://wmg-streaming-prediction-dev/streaming_data_processed/total_streams_by_artist.csv'\n",
    "df_artist_total_streams = create_num_streams_by_artist(df_streaming_lim, df_product_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 5, 6, & 7: day 1, day 2 and day 3 streams per track after release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first3days_volume(df):\n",
    "    # Restrict to rows where stream date and release date are equal, 1 day apart, 2 days apart, 3 days apart\n",
    "    mask = ((df['stream_date']-df['release_date']==datetime.timedelta(days=0)) |\n",
    "            (df['stream_date']-df['release_date']==datetime.timedelta(days=1)) |\n",
    "            (df['stream_date']-df['release_date']==datetime.timedelta(days=2)) |\n",
    "            (df['stream_date']-df['release_date']==datetime.timedelta(days=3)))\n",
    "\n",
    "    df_streaming_dayvolume = df[mask]\n",
    "\n",
    "    # Create helper variables for each day\n",
    "    df_streaming_dayvolume['day0'] = df_streaming_dayvolume['stream_date']==df_streaming_dayvolume['release_date']\n",
    "    df_streaming_dayvolume['day1'] = df_streaming_dayvolume['stream_date']==(df_streaming_dayvolume['release_date']+datetime.timedelta(days=1))\n",
    "    df_streaming_dayvolume['day2'] = df_streaming_dayvolume['stream_date']==(df_streaming_dayvolume['release_date']+datetime.timedelta(days=2))\n",
    "    df_streaming_dayvolume['day3'] = df_streaming_dayvolume['stream_date']==(df_streaming_dayvolume['release_date']+datetime.timedelta(days=3))\n",
    "    \n",
    "    print('Shape of daily streaming data:\\t{}'.format(df_streaming_dayvolume.shape))\n",
    "    \n",
    "    # Partition into different dataframes and rename cols\n",
    "    day0 = df_streaming_dayvolume[df_streaming_dayvolume['day0']]\n",
    "    day0 = day0[['product_key','ISRC','streams']]\n",
    "    day0 = day0.rename(columns={'streams':'day0_volume'})\n",
    "    print('Number of songs with day 0 data:\\t{}'.format(len(day0)))\n",
    "\n",
    "    day1 = df_streaming_dayvolume[df_streaming_dayvolume['day1']]\n",
    "    day1 = day1[['product_key','ISRC','streams']]\n",
    "    day1 = day1.rename(columns={'streams':'day1_volume'})\n",
    "    print('Number of songs with day 1 data:\\t{}'.format(len(day1)))\n",
    "\n",
    "    day2 = df_streaming_dayvolume[df_streaming_dayvolume['day2']]\n",
    "    day2 = day2[['product_key','ISRC','streams']]\n",
    "    day2 = day2.rename(columns={'streams':'day2_volume'})\n",
    "    print('Number of songs with day 2 data:\\t{}'.format(len(day2)))\n",
    "\n",
    "    day3 = df_streaming_dayvolume[df_streaming_dayvolume['day3']]\n",
    "    day3 = day3[['product_key','ISRC','streams']]\n",
    "    day3 = day3.rename(columns={'streams':'day3_volume'})\n",
    "    print('Number of songs with day 3 data:\\t{}'.format(len(day3)))\n",
    "\n",
    "    # Merge everything together\n",
    "    df_daily_volumes=day0.merge(day1[['ISRC','day1_volume']].merge(day2[['ISRC','day2_volume']].merge(day3[['ISRC','day3_volume']],on='ISRC',how='outer'),on='ISRC',how='outer'),on='ISRC',how='outer')\n",
    "    df_daily_volumes.drop(columns=['product_key'],inplace=True)\n",
    "\n",
    "    print('Shape of final data frame:\\t{}'.format(df_daily_volumes.shape))\n",
    "    \n",
    "    return df_daily_volumes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of daily streaming data:\t(196016, 9)\n",
      "Number of songs with day 0 data:\t46759\n",
      "Number of songs with day 1 data:\t49296\n",
      "Number of songs with day 2 data:\t49535\n",
      "Number of songs with day 3 data:\t50426\n",
      "Shape of final data frame:\t(52543, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df_daily_volumes = create_first3days_volume(df_streaming_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 8, 9 & target variable: Artist previous release streams, artist previous release first week streams, and target variable: song's first week streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_sum(x):\n",
    "    #helper function to get the first seven numbers' sum of a column.\n",
    "    return x.head(7).sum()\n",
    "\n",
    "def create_artist_prev_avg_streams(stream_df, prod_df):\n",
    "    #input: cleaned streaming table and product dim table\n",
    "    #output: \n",
    "    #feature 8: avg_stream_per_project(dataframe): artist's previous release's streams\n",
    "    #feature 9: first_week_streams(dataframe): artist's previous release's first week streams\n",
    "    #target variable: current song's first week's stream volumes\n",
    "    \n",
    "    # add project and artist related information to streaming data, and clean the streaming table\n",
    "    stream_with_project = stream_df.merge(prod_df[['product_key', 'project_key', 'product_title', 'artist_id']], how = 'left', on = 'product_key')\n",
    "    #remove null artist values and and artist not recorded in product table\n",
    "    stream_with_project = stream_with_project[~stream_with_project['artist_id'].isnull()]\n",
    "    stream_with_project = stream_with_project[stream_with_project['artist_id'].isin(prod_df['artist_id'])]\n",
    "    stream_with_project['stream_date_datetime'] = pd.to_datetime(stream_with_project['stream_date'])\n",
    "    stream_with_project['day_of_week'] = stream_with_project['stream_date_datetime'].dt.dayofweek\n",
    "    \n",
    "    #feature 8: artist most recent previous release's streams. (calculated as total streams up to date of the previous release by artist).\n",
    "    # note that if the artist's last release has more than one song (more than 1 unique ISRC at the same release date and with the same project id), then we calculate an average of the these songs.\n",
    "    # get total streams per song\n",
    "    total_stream_per_product = stream_with_project.groupby('product_key').agg({'artist_id': 'max', 'project_key': 'max', 'product_title': 'max', 'release_date': 'max', 'streams': 'sum'}).reset_index()\n",
    "    # get average streams per project with the same release date and same artist, and all the song titles for the current project \n",
    "    avg_stream_per_project = total_stream_per_product.groupby(['artist_id', 'project_key', 'release_date']).agg({'streams': 'mean',  'product_title': (lambda column: \"|\".join(column))}).reset_index()\n",
    "    # get average streams per project with the same release date and same artist, and all the song titles from last release\n",
    "    avg_stream_per_project['shifted_stream'] = avg_stream_per_project.sort_values(['artist_id','release_date'], ascending=True).groupby(['artist_id'])['streams'].shift(1)\n",
    "    avg_stream_per_project['shifted_prod_title'] = avg_stream_per_project.sort_values(['artist_id','release_date'], ascending=True).groupby(['artist_id'])['product_title'].shift(1)\n",
    "    avg_stream_per_project['shifted_project_key'] = avg_stream_per_project.sort_values(['artist_id','release_date'], ascending=True).groupby(['artist_id'])['project_key'].shift(1)\n",
    "    avg_stream_per_project['shifted_stream'].fillna(0, inplace = True) ## fill na with 0: assuming no album before\n",
    "    avg_stream_per_project.columns = ['artist_id', 'project_key', 'release_date', 'streams', 'product_title', 'previous_release_avg_streams', 'prev_prod_title', 'prev_project_key']\n",
    "    \n",
    "    #feature 9: artist most recent previous release's first week streams. (calculated as first seven days' streams since release of the previous release by artist).\n",
    "    # note that if the artist's last release has more than one song (more than 1 unique ISRC at the same release date and with the same project id), then we calculate an average of the these songs.\n",
    "    stream_with_project = stream_with_project.sort_values(['product_key','stream_date_datetime'], ascending=True)\n",
    "    column_funcs = {'streams': head_sum, # first seven days sum\n",
    "                'project_key': max, # all the vals should be the same\n",
    "                'artist_id': max, # all the vals should be the same\n",
    "                'release_date': max }  # all the vals should be the same\n",
    "    collapsed = stream_with_project.groupby('product_key').aggregate(column_funcs)\n",
    "    first_week_streams = collapsed.reset_index().sort_values(['streams'], ascending=False)\n",
    "    first_week_streams.columns = ['product_key', 'first_week_streams', 'project_key', 'artist_id', 'release_date']\n",
    "    # get average first week streams per project with the same release date and same artist, and all the song titles for the current project \n",
    "    first_week_streams = first_week_streams.groupby(['artist_id', 'project_key', 'release_date']).agg({'first_week_streams': 'mean'}).reset_index()\n",
    "    # get average first week streams per project with the same release date and same artist, and all the song titles from last release\n",
    "    first_week_streams['previous_release_first_week_stream'] = first_week_streams.sort_values(['artist_id','release_date'], ascending=True).groupby(['artist_id'])['first_week_streams'].shift(1)\n",
    "    first_week_streams['previous_release_first_week_stream'].fillna(0, inplace = True) ## fill na with 0: assuming no album before\n",
    "    \n",
    "    # creat target variable: each song's first week total streams\n",
    "    first_week_streams_target = collapsed.reset_index().sort_values(['streams'], ascending=False)\n",
    "    first_week_streams_target.columns = ['product_key', 'first_week_streams', 'project_key', 'artist_id', 'release_date']\n",
    "    #del stream_with_project\n",
    "    return avg_stream_per_project, first_week_streams, first_week_streams_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prev_streams, avg_prev_first_week_streams, target_var = create_artist_prev_avg_streams(df_streaming_lim, df_product_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all created features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist_id                               object\n",
       "project_key                             object\n",
       "release_date                    datetime64[ns]\n",
       "streams                                float64\n",
       "product_title                           object\n",
       "previous_release_avg_streams           float64\n",
       "prev_prod_title                         object\n",
       "prev_project_key                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prev_streams.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49294, 14)\n",
      "(49294, 15)\n",
      "(49294, 17)\n",
      "(44036, 21)\n",
      "(44036, 24)\n",
      "(44036, 25)\n",
      "(44036, 26)\n"
     ]
    }
   ],
   "source": [
    "# Combine all files\n",
    "print(df_product_lim.shape)\n",
    "df_final = df_product_lim.merge(df_artist_total_streams,\n",
    "                                     on='artist_id',\n",
    "                                     how='inner')\n",
    "print(df_final.shape)\n",
    "\n",
    "df_final = df_final.merge(df_numtracks,\n",
    "                          on='artist_id',\n",
    "                          how='inner')\n",
    "print(df_final.shape)\n",
    "\n",
    "df_final = df_final.merge(df_daily_volumes,\n",
    "                          on='ISRC',\n",
    "                          how='inner')\n",
    "print(df_final.shape)\n",
    "df_final['release_date'] = pd.to_datetime(df_final['release_date'])\n",
    "df_final = df_final.merge(avg_prev_streams[['project_key', 'release_date',  'previous_release_avg_streams', 'prev_prod_title', 'prev_project_key']],\n",
    "                          how = 'left', \n",
    "                          on = ['project_key', 'release_date'])\n",
    "print(df_final.shape)\n",
    "\n",
    "df_final = df_final.merge(avg_prev_first_week_streams[['project_key', 'release_date', 'previous_release_first_week_stream']], \n",
    "                          how = 'left', \n",
    "                          on = ['project_key', 'release_date'])\n",
    "print(df_final.shape)\n",
    "\n",
    "df_final = df_final.merge(target_var[['product_key', 'first_week_streams']],\n",
    "                                  how = 'left', \n",
    "                                  on = 'product_key')\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding categorical feature: genre\n",
    "to keep the categorical feature size down, we filtered out genres with less than a few tens songs, and merged same/similar genre terms together based on our understanding. This mapping can be modified in the future by domain expert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_genre(feature_df):\n",
    "    # input: merged feature dataframe.\n",
    "    # output: merged feature dataframe with genre converted to one hot encoded feature columns.\n",
    "    #filter out genres with less than a few tens songs\n",
    "    genre_filter_out = feature_df.groupby(['major_genre_code']).count().reset_index().sort_values(by = 'product_key', ascending = False)['major_genre_code'][-19:]\n",
    "    df_final_feature = feature_df[~feature_df['major_genre_code'].isin(genre_filter_out)]\n",
    "    # create a mapping to merged same/similar genre terms together\n",
    "    equiv = {2697: 2714, 2698: 2699, 2700:2699, 2707: 1000,2717: 2744, 2719: 1015685, 2727: 2710, 2728: 1000, 2738: 1000, 2743: 2767, 2750: 1001, \n",
    "            2752: 1000, 2756: 2755, 2768: 2744, 2774: 2744, 3933: 2699, 1000127: 2734,1000128: 2734, 1000129: 2734, 1000130: 2734, 1000328: 1000,\n",
    "            1000386: 1001, 1008945:1000, 1009585:1001, 1009905:1001, 1011382:1001, 1011385:2744,  1013145:2767, 1014465: 1001, 1014965:1001, 1006386: 1001,\n",
    "            2705: 2732, 2771: 2767, 2710: 2744, 2724: 2744}\n",
    "    df_final_feature['genre_mapped'] = df_final_feature['major_genre_code'].map(equiv)\n",
    "    df_final_feature['genre_mapped'].fillna(df_final_feature['major_genre_code'], inplace = True) # for songs we are not able to find a match, use orginal genre code\n",
    "    df_final_feature = pd.concat([df_final_feature,pd.get_dummies(df_final_feature['genre_mapped'], prefix='genre')],axis=1)\n",
    "    return df_final_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "df_final = process_genre(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power transform total stream for the artist \n",
    "to normalize and bucket each artist's total stream; also avoid data leaking. You can also self-define artist's bucket with a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate \n",
    "pt = PowerTransformer(method='box-cox', standardize=False,) \n",
    "\n",
    "#Fit the data to the powertransformer\n",
    "scale = pt.fit(df_final[['streams']])\n",
    "\n",
    "#Transform the data. you should transform the future data with the same scale above.\n",
    "scaled_stream = pt.transform(df_final[['streams']])\n",
    "df_final['total_streams_boxcox'] = scaled_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final data quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.fillna(0) # fill null values with 0 \n",
    "#only keep data with first three day's volume bigger than 0\n",
    "df_final = df_final[(df_final['day0_volume'] > 0) & (df_final['day1_volume'] > 0) & (df_final['day2_volume'] > 0)]\n",
    "#only look at songs with first week's stream bigger than 200\n",
    "df_final = df_final[df_final['first_week_streams'] > 200]\n",
    "# clean out columns not for training\n",
    "df_final.rename(columns = {'Collaboration': 'has_collaboration'}, inplace = True)\n",
    "df_final =df_final[['product_key', \n",
    "        'has_collaboration',\n",
    "       'day0_volume', 'day1_volume', 'day2_volume', \n",
    "       'previous_release_avg_streams', 'genre_mapped',\n",
    "       'previous_release_first_week_stream', 'first_week_streams',\n",
    "        'total_streams_boxcox', 'artist_num_tracks',\n",
    "       'genre_1000128', 'genre_1000345', 'genre_1000346',\n",
    "       'genre_1009046', 'genre_1009585', 'genre_1009905', 'genre_1011385',\n",
    "       'genre_1014465', 'genre_1014965', 'genre_1015685', 'genre_2698',\n",
    "       'genre_2699', 'genre_2700', 'genre_2705', 'genre_2706', 'genre_2708',\n",
    "       'genre_2709', 'genre_2711', 'genre_2714', 'genre_2719', 'genre_2724',\n",
    "       'genre_2727', 'genre_2729', 'genre_2732', 'genre_2734', 'genre_2736',\n",
    "       'genre_2744', 'genre_2751', 'genre_2753', 'genre_2754', 'genre_2755',\n",
    "       'genre_2758', 'genre_2767', 'genre_2774']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_location = 's3://wmg-streaming-prediction-dev/streaming_data_processed'\n",
    "df_final.to_csv('{}/df_final_feature_v4.csv'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.sample(frac=1, random_state=1729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df_final.groupby('genre_mapped')\n",
    "arr_list = [np.split(g, [int(.7 * len(g)), int(.9 * len(g))]) for i, g in grouped_df]\n",
    "\n",
    "train_data = pd.concat([t[0] for t in arr_list])\n",
    "validation_data = pd.concat([t[1] for t in arr_list])\n",
    "test_data  = pd.concat([v[2] for v in arr_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, name):\n",
    "    data = data.drop(columns = ['product_key', 'genre_mapped'])\n",
    "    data = pd.concat([data['first_week_streams'], data.drop(['first_week_streams'], axis=1)], axis=1) \n",
    "    data.to_csv(name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(train_data, 'train_updated.csv')\n",
    "process_data(validation_data, 'validation_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train_updated.csv')\n",
    "s3_input_validation = boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation_updated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
