{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing and Feature Engineering \n",
    "### For Time Series model\n",
    "This Notebook will process the created data and create features for the regression model. The Following data sources are used: <br>\n",
    "dim_product: product fact table <br>\n",
    "streaming: aggregated daily streaming data (2018/6/1/-2020/7/2) <br>\n",
    "Note: New features can be added as a series or catagorical variable. Ideally, the prepcrocessing and feature engineering are preferred to be completed in a ETL tool before data arrives in Sagemaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages and set envionment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (4.44.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a random seed to have a definite set of train/validation set\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "#set parameters\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_bucket = 'wmg-streaming-prediction-dev/streaming_data_processed' \n",
    "s3_prefix = 'ts_data'   \n",
    "\n",
    "role = sagemaker.get_execution_role()  \n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in streaming data and reference data for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to apply the same filter and keep same amount of songs for training as our regression model, so we use processed regression model features as a references.\n",
    "ref_data_name = 's3://wmg-streaming-prediction-dev/streaming_data_processed/df_final_feature_v2.csv'\n",
    "ref_data = pd.read_csv(ref_data_name, index_col = False)\n",
    "equiv = {2697: 2714, 2698: 2699, 2700:2699, 2707: 1000,2717: 2744, 2719: 1015685, 2727: 2710, 2728: 1000, 2738: 1000, 2743: 2767, 2750: 1001, \n",
    "        2752: 1000, 2756: 2755, 2768: 2744, 2774: 2744, 3933: 2699, 1000127: 2734,1000128: 2734, 1000129: 2734, 1000130: 2734, 1000328: 1000,\n",
    "        1000386: 1001, 1008945:1000, 1009585:1001, 1009905:1001, 1011382:1001, 1011385:2744,  1013145:2767, 1014465: 1001, 1014965:1001, 1006386: 1001,\n",
    "        2705: 2732, 2771: 2767, 2710: 2744, 2724: 2744}\n",
    "ref_data['genre_mapped'] = ref_data['major_genre_code'].map(equiv)\n",
    "ref_data['genre_mapped'].fillna(ref_data['major_genre_code'], inplace = True)\n",
    "ref_data = ref_data[(ref_data['day0_volume'] != 0) & (ref_data['day1_volume'] != 0) & (ref_data['day2_volume'] != 0)]\n",
    "ref_data = ref_data[(ref_data['first_week_streams']>200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in raw streaming files. we want to use all history of a song for training\n",
    "raw = pd.read_csv('s3://wmg-streaming-prediction-dev/product-key-agg/rna.gz', compression='gzip', \n",
    "                  # names = col_names,\n",
    "                  # sep='\\x01',\n",
    "                   quotechar='\"')\n",
    "\n",
    "raw.columns = ['product_key', 'ISRC', 'release_date', 'stream_date', 'streams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep same songs for two models\n",
    "filtered_data = raw[raw['product_key'].isin(ref_data['product_key'])]\n",
    "del raw # run this commend if OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert the all songs have data since release date\n",
    "check_date = filtered_data.groupby('product_key').agg({'release_date': 'min', 'stream_date':'min'})\n",
    "check_date['release_date'].equals(check_date['stream_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format check and order the dataframe as timeseries\n",
    "filtered_data['stream_date_datetime'] = pd.to_datetime(filtered_data['stream_date']).dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "filtered_data = filtered_data.sort_values(['product_key','stream_date_datetime'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create time series input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29855/29855 [07:37<00:00, 65.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# create time series following the DeepAR input format\n",
    "timeseries = []\n",
    "prod_key_list = filtered_data['product_key'].unique()\n",
    "for i in tqdm(prod_key_list):\n",
    "    ts_i = filtered_data[filtered_data['product_key'] == i]\n",
    "    idx = pd.period_range(min(ts_i.stream_date_datetime), max(ts_i.stream_date_datetime))\n",
    "    ts_i_reindexed = ts_i.set_index('stream_date_datetime')['streams']\n",
    "    ts_i_reindexed = ts_i_reindexed.rename(str(i))\n",
    "    ts_i_reindexed.index = pd.to_datetime(ts_i_reindexed.index)\n",
    "    ts_i_reindexed = ts_i_reindexed.asfreq(freq = 'D')\n",
    "    timeseries.append(ts_i_reindexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_data # run this commend if OOM error\n",
    "## we want to keep songs with longer than 60 days history for training\n",
    "cutted_ts  = [ts for ts in timeseries if len(ts) <= 60]\n",
    "timeseries = [ts for ts in timeseries if len(ts) > 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold out a sample for testing:\n",
    "# do a stratified train/test split to make sure each genre has at least some songs represented in both training and testing set.\n",
    "X = pd.concat([ref_data.iloc[:, :12], ref_data.iloc[:, 13:]], axis = 1)\n",
    "y = ref_data.first_week_streams\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    train_size=.8, \n",
    "                                                    stratify=ref_data.major_genre_code)\n",
    "#keep training data for modeling, and test data for testing\n",
    "modeling_list = list(x_train['product_key'].apply(str))\n",
    "cutoff_list = list(x_test['product_key'].apply(str))\n",
    "timeseries_modeling = [ts for ts in timeseries if (ts.name) in modeling_list]\n",
    "timeseries_cutoff = [ts for ts in timeseries if (ts.name) in cutoff_list]\n",
    "# data format check\n",
    "timeseries_modeling = [ts.astype(float).fillna(0) for ts in timeseries_modeling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5604/5604 [00:10<00:00, 549.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#process and save text files\n",
    "timeseries_cutoff = [ts.astype(float).fillna(0) for ts in timeseries_cutoff]\n",
    "ts_cat_cutoff = []\n",
    "for ts in tqdm(timeseries_cutoff):\n",
    "    prod_key = ts.name \n",
    "    cat_code = ref_data[ref_data['product_key'] == int(prod_key)][['genre_coded', 'has_collaboration']]\n",
    "    cat_code_array = cat_code.iloc[0].rename(prod_key)\n",
    "    ts_cat_cutoff.append(cat_code_array) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and save text files\n",
    "with open(\"ts_cut_off.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(timeseries_cutoff, fp)\n",
    "with open(\"ts_cat_cut_off.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(ts_cat_cutoff, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to save files to local and s3\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "\n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Genre as a Catagorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22284/22284 [00:40<00:00, 546.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a genre map following DeepAR input Format\n",
    "genre_map = {}\n",
    "for i, genre in enumerate(ref_data['genre_mapped'].unique()):\n",
    "    genre_map.update({genre: i})\n",
    "ref_data['genre_coded'] = ref_data['genre_mapped'].map(genre_map)\n",
    "\n",
    "# create a series of categorical variable \n",
    "ts_cat_modeling = []\n",
    "for ts in tqdm(timeseries_modeling):\n",
    "    prod_key = ts.name \n",
    "    cat_code = ref_data[ref_data['product_key'] == int(prod_key)][['genre_coded', 'has_collaboration']]\n",
    "    cat_code_array = cat_code.iloc[0].rename(prod_key)\n",
    "    ts_cat_modeling.append(cat_code_array)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters to create training and testing files\n",
    "freq = 'D'\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 \n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training and testing data with genre feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_new_features = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0]),\n",
    "        \"target\": ts[:-prediction_length*2].tolist(),\n",
    "        \"cat\": ts_cat_modeling[i].tolist()\n",
    "    }\n",
    "    for i, ts in enumerate(timeseries_modeling)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 2\n",
    "test_data_new_features = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0]),\n",
    "        \"target\": ts[:-prediction_length*k].tolist(),\n",
    "        \"cat\": ts_cat_modeling[i].tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for i, ts in enumerate(timeseries_modeling)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data consistency, quality check before saving\n",
    "def check_dataset_consistency(train_dataset, test_dataset=None):\n",
    "    d = train_dataset[0]\n",
    "    has_dynamic_feat = 'dynamic_feat' in d\n",
    "    if has_dynamic_feat:\n",
    "        num_dynamic_feat = len(d['dynamic_feat'])\n",
    "    has_cat = 'cat' in d\n",
    "    if has_cat:\n",
    "        num_cat = len(d['cat'])\n",
    "    \n",
    "    def check_ds(ds):\n",
    "        for i, d in enumerate(ds):\n",
    "            if has_dynamic_feat:\n",
    "                assert 'dynamic_feat' in d\n",
    "                assert num_dynamic_feat == len(d['dynamic_feat'])\n",
    "                for f in d['dynamic_feat']:\n",
    "                    assert len(d['target']) == len(f)\n",
    "            if has_cat:\n",
    "                assert 'cat' in d\n",
    "                assert len(d['cat']) == num_cat\n",
    "    check_ds(train_dataset)\n",
    "    if test_dataset is not None:\n",
    "        check_ds(test_dataset)\n",
    "        \n",
    "check_dataset_consistency(training_data_new_features, test_data_new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to S3 this may take a few minutes depending on your connection.\n",
      "Overwriting existing file\n",
      "Uploading file to s3://wmg-streaming-prediction-dev/streaming_data_processed/ts_data-new-features/data/train/train_new_features.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://wmg-streaming-prediction-dev/streaming_data_processed/ts_data-new-features/data/test/test_new_features.json\n"
     ]
    }
   ],
   "source": [
    "write_dicts_to_file(\"train_new_features.json\", training_data_new_features)\n",
    "write_dicts_to_file(\"test_new_features.json\", test_data_new_features)\n",
    "\n",
    "s3_data_path_new_features = \"s3://{}/{}-new-features/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path_new_features = \"s3://{}/{}-new-features/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "print('Uploading to S3 this may take a few minutes depending on your connection.')\n",
    "copy_to_s3(\"train_new_features.json\", s3_data_path_new_features + \"/train/train_new_features.json\", override=True)\n",
    "copy_to_s3(\"test_new_features.json\", s3_data_path_new_features + \"/test/test_new_features.json\", override=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
